<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">

  <title>Intro to Apache Spark</title>

  <meta name="description" content="Apache Spark is a fast and general engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing.">
  <meta name="author" content="Amine Sagaama">

  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

  <link rel="stylesheet" href="css/reveal.min.css">
  <link rel="stylesheet" href="css/theme/default.css" id="theme">

  <!-- For syntax highlighting -->
  <link rel="stylesheet" href="lib/css/zenburn.css">

  <!-- If the query includes 'print-pdf', use the PDF print sheet -->
  <script>
  document.write( '<link rel="stylesheet" href="css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
  </script>

  <!--[if lt IE 9]>
  <script src="lib/js/html5shiv.js"></script>
  <![endif]-->

  <style type="text/css">
  .reveal .controls {
    bottom: auto;
    top: 0px;
  }
  </style>

</head>

<body style="transition: -webkit-transform 0.8s ease;">

  <aside style="display: block; position: fixed; bottom: 5px; left: 10px; z-index: 30;">
    <a href="http://ebiznext.com"><img src="assets/logoebiz.jpeg" width="60" height="50"></a>
  </aside>

  <aside style="display: block; position: fixed; bottom: 10px; right: 10px; z-index: 30;">
    <a href="https://twitter.com/amsagaama" class="twitter-follow-button" data-show-count="false" data-lang="en" data-size="large">Follow @amsagaama</a>
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
  </aside>

  <div class="reveal">

    <div class="slides">

      <section>
        <h2>GETTING STARTED WITH APACHE SPARK</h2>
        <h4>LIGHTNING-FAST CLUSTER COMPUTING</h4>
        <p>
          <small>
          <a href="https://www.linkedin.com/in/aminesagaama/">Amine Sagaama</a>
               | 
          <a href="https://twitter.com/amsagaama">@amsagaama</a>
          </small>
          <br>
          <small>
          <em>Data Engineer @ Ebiznext.com</em>
          </small>
        </p>
      </section>

      <section>
        <h2>WHAT IS APACHE SPARK ?</h2>

        <ul>
          <li class="fragment">A cluster computing platform designed to be fast and general-purpose</li>
          <li class="fragment">Extends the popular MapReduce model to efficiently support more types of computations</li>
          <li class="fragment">Ability to run computations in memory</li>
          <li class="fragment">Spark offers simple APIs in Python, Java, Scala,and SQL</li>
        </ul>
      </section>

      <section>
        <h2>THE SPARK STACK</h2>

        <img src="assets/spark_stack.png" width="70%" />
      </section>

      <section>
        <h2>SPARK CORE</h2>

        <blockquote>Spark Core contains the basic functionality of Spark</blockquote>
         <br>
        <ul>
          <li>Task scheduling</li>
          <li>Memory management</li>
          <li>Ability to run computations in memory</li>
          <li>Fault recovery</li>
        </ul>
      </section>

      <section>
        <h2>Spark SQL</h2>

        <blockquote>Spark SQL is Spark’s package for working with structured data</blockquote>
         <br>
        <ul>
          <li>Allows querying data via SQL / Apache Hive</li>
          <li>Supports many sources of data : Hive tables / Parquet / JSON</li>
          <li>Provides SQL interface to Spark</li>
          <li>Fault recovery</li>
        </ul>
      </section>

      <section>
        <h2>Spark Streaming</h2>

        <blockquote>Spark Streaming is a Spark component that enables processing of live streams of data</blockquote>
         <br>
        <ul>
          <li>Logfiles generated by production web servers</li>
          <li>Queues of messages containing status updates posted by users of a web service</li>
        </ul>
      </section>

      <section>
        <h2>MLlib</h2>

        <blockquote>MLlib provides multiple types of machine learning algorithms</blockquote>
         <br>
        <ul>
          <li>Classification</li>
          <li>Regression</li>
          <li>Clustering</li>
          <li>Collaborative filtering</li>
        </ul>
      </section>

      <section>
        <h2>GraphX</h2>

        <blockquote>GraphX is a library for manipulating graphs and performing graph-parallel computations</blockquote>
         <br>
        <ul>
          <li>Extends the Spark RDD API</li>
          <li>Provides various oper‐ators for manipulating graphs : subgraph / mapVertices</li>
        </ul>
      </section>

      <section>
        <h2>Getting Started</h2>

        <ol style="width: 90%">
          <li>Download Apache Spark: <a href="http://spark.apache.org/downloads.html">spark.apache.org/downloads.html</a></li>
          <li>Extract Spark:<br/><pre><code>cd ~
tar -xf spark-2.1.1-bin-hadoop2.7.tgz
cd spark-2.1.1-bin-hadoop2.7
ls</code></pre></li>
          <li>Launch Spark (Python version of the Spark shell) :<br/><pre><code>bin/pyspark</code></pre></li>
          <li>Launch Spark (cala version of the shell) :<br/><pre><code>bin/spark-shell</code></pre></li>
          <li>Let’s walk through the example from the: <a href="http://spark.apache.org/docs/latest/quick-start.html">spark.apache.org/docs/latest/quick-start.html</a></li>
        </ol>
      </section>

      <section>
        <h2>Scala line count</h2>

        <p>
          <pre><code>scala> val lines = sc.textFile("README.md") // Create an RDD called lines
lines:spark.RDD[String] = MappedRDD[...]</code></pre>
        </p>

        <p>
          <pre><code>scala> lines.count() // Count the number of items in this RDD
res0: Long = 127</code></pre>
        </p>

        <p>
          <pre><code>scala> lines.first() // First item in this RDD, i.e. first line of README.md
res1 : String = #ApacheSpark</code></pre>
        </p>
      </section>

      <section>
        <h2>Scala filtering example</h2>

        <p>
          <pre><code>scala> val lines = sc.textFile("README.md") // Create an RDD called lines
lines : spark.RDD[String] = MappedRDD[...]</code></pre>
        </p>

        <p>
          <pre><code>scala> val pythonLines = lines.filter(line => line.contains("Python"))
pythonLines : spark.RDD[String] = FilteredRDD[...]</code></pre>
        </p>

        <p>
          <pre><code>scala> pythonLines.first()
res0 : String = ## InteractivePythonShell</code></pre>
        </p>
      </section>

      <section>
        <h2>Standalone Applications</h2>

        <h4>Initializing a SparkContext</h4>

        <p>
          <pre><code>import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

val conf = new SparkConf().setMaster("local").setAppName("My App")
val sc = new SparkContext(conf)</code></pre>
        </p>

        <ul>
          <li>A cluster URL "local" which tells Spark how to connect to a cluster</li>
          <li>An application name "My App" this will identify your application on the cluster manager’s UI if you connect to a cluster</li>
        </ul>
        
      </section>

      <section>
        <h2>Building Standalone Applications</h2>

        <p>
        Word count Scala application
          <pre><code>// Create a Scala Spark Context
val conf = new SparkConf().setAppName("wordCount")
val sc = new SparkContext(conf)
// Load our input data.
val input = sc.textFile(inputFile)
// Split it up into words.
val words = input.flatMap(line => line.split(" "))
// Transform into pairs and count.
val counts = words.map( word => (word,1)).reduceByKey{ case(x,y) => x + y}
// Save the word count back out to a text file, causing evaluation
counts.saveAsTextFile(outputFile)</code></pre>
        </p>

        sbt build file
        <p>
          <pre><code>name := "spark-example"

version := "0.0.1"

scalaVersion := "2.11.1"

libraryDependencies ++= Seq(
"org.apache.spark"%%"spark-core"%"2.11"%"provided"
)</code></pre>
        </p>
        
      </section>

      <section>
        <h2>Learn More</h2>

        <ul>
          <li>Slides available on <a href="https://aminesagaama.github.io/quick-start-spark">https://aminesagaama.github.io/quick-start-spark</a></li>
          <li><a href="http://spark.apache.org/docs/latest/programming-guide.html">Spark Programming Guide</a></li>
          <li><a href="http://spark.apache.org/docs/latest/cluster-overview.html">Cluster Mode Overview</a></li>
          <li><a href="http://spark.apache.org/docs/latest/submitting-applications.html">Submitting Applications</a></li>
        </ul>
      </section>

  </div>

</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.min.js"></script>

<script>

// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
  controls: true,
  progress: true,
  history: true,
  center: true,

  theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
  transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

  // Optional libraries used to extend on reveal.js
  dependencies: [
    { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
    { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
    { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
    { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
    { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
  ]
});

</script>

</body>
</html>
